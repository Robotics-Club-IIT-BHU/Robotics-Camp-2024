ONNX offers a standardised method for developers to define and exchange deep learning models across various frameworks like TensorFlow, PyTorch, and more.

### Intuition
Suppose we train a model in the PyTorch framework. After training, we usually save that model in a file, such as `model.pt`, so that we can load the model from this file in the future and use it without retraining. However, this `model.pt` file is specific to PyTorch, meaning it can only be used in the PyTorch framework. If we wanted to use this model in TensorFlow, it would not be possible.
Similarly, we save models trained in TensorFlow in `model.tf` files, which can only be used within the TensorFlow framework.

This is where ONNX comes in. Instead of saving the model in PyTorch as a `model.pt` file, we can save the model as an `.onnx` file. This `model.onnx` file can then be used in any framework that supports ONNX.


<div align="center">

    <img src="media/onnx_multitransfer.png" height="70%" width="70%">

  <br>
</div>

In simpler terms, ONNX is a generalized file format that allows different machine learning frameworks to save their models and also use models trained on other frameworks

## ONNX-Runtime

If you have a `model.onnx` file, you can load that model into your desired framework and get predictions from it. However, the most optimized and widely accepted way to load an ONNX model is to use the ONNX Runtime.

```python
!pip install onnxruntime
!pip install onnx

import onnxruntime as ort
import numpy as np

# Load the ONNX model
model_path = '/content/drive/MyDrive/my projects/yoloonnx/tinyyolo-v1.3-o8.onnx'
session = ort.InferenceSession(model_path)

# Get model metadata
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

# Prepare input data
# Assuming the model expects a single input of shape [batch_size, channels, height, width]
# Create a dummy input for demonstration purposes
input_data = np.random.rand(1, 3, 416, 416).astype(np.float32)

# Run inference
result = session.run([output_name], {input_name: input_data})
# Get the predictions
predictions = result[0]
print(predictions.shape)

# output => (1, 125, 13, 13)
```



